{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T14:48:28.400642Z","iopub.execute_input":"2022-06-06T14:48:28.401009Z","iopub.status.idle":"2022-06-06T14:48:28.436484Z","shell.execute_reply.started":"2022-06-06T14:48:28.400917Z","shell.execute_reply":"2022-06-06T14:48:28.435737Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df0 = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n\ntest_df2 = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n\ntest_df2_copy = test_df2.copy()\n\ndf0.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:28.437891Z","iopub.execute_input":"2022-06-06T14:48:28.438263Z","iopub.status.idle":"2022-06-06T14:48:28.575222Z","shell.execute_reply.started":"2022-06-06T14:48:28.438231Z","shell.execute_reply":"2022-06-06T14:48:28.574201Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"However, before doing anything lets do something very important first - split our data into training and test sets. It is important to do it early on (before looking into the data too much) as we may end up focusing on specific patterns in our data which lead us to build/focus on models that do not work well on new data. If we do this, we will not have a good idea of how our model will perform in practice (generalisability). So we only want to work with our \"training\" data, and at the very end when we are happy with our model, have a look at its performance on the \"test\" data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf, test_df = train_test_split(df0,          # data to split \n                               test_size=0.2,# we will leave 20% to test our models on later \n                               random_state=1234,  # make our work reproducable \n                               shuffle = True)     # prevent data ordering affecting our model\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:28.576769Z","iopub.execute_input":"2022-06-06T14:48:28.577096Z","iopub.status.idle":"2022-06-06T14:48:29.926185Z","shell.execute_reply.started":"2022-06-06T14:48:28.577045Z","shell.execute_reply":"2022-06-06T14:48:29.925267Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Reset index to to prevent problems caused by the wrong use of .loc or .iloc \n\ndf = df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:29.928847Z","iopub.execute_input":"2022-06-06T14:48:29.929387Z","iopub.status.idle":"2022-06-06T14:48:29.963103Z","shell.execute_reply.started":"2022-06-06T14:48:29.929340Z","shell.execute_reply":"2022-06-06T14:48:29.961955Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:29.964803Z","iopub.execute_input":"2022-06-06T14:48:29.965120Z","iopub.status.idle":"2022-06-06T14:48:29.973995Z","shell.execute_reply.started":"2022-06-06T14:48:29.965080Z","shell.execute_reply":"2022-06-06T14:48:29.972929Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(figsize=(5,5))\n\n# Defind colours for print()\nclass color:\n    PURPLE = '\\033[95m'\n    CYAN = '\\033[96m'\n    DARKCYAN = '\\033[36m'\n    BLUE = '\\033[94m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    END = '\\033[0m'\n    \nprint(color.BOLD+color.UNDERLINE+\"Transported Status (%)\"+color.END)\n((df['Transported'].value_counts()/len(df['Transported']))*100).round(2)\n\n# 0 = Not Transported \n# 1 = Transported \n\ndefault_status_count=[len(df[df.Transported==1]),len(df[df.Transported==0])]\n\nax=plt.pie(x = default_status_count, labels=['Transported', 'Not Transported '],\n           autopct='%1.1f%%',colors = ['#3192C1', '#A9DDB5'], pctdistance=.77, startangle=45, textprops={\"fontsize\":12},\n           wedgeprops={'edgecolor':'#06113C'}\n)\n\nplt.legend(['Transported', 'Not-Transported'], bbox_to_anchor=(.8, .99))\n\ncentre_circle = plt.Circle((0,0),0.6,fc='white', ec='#06113C')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:29.975931Z","iopub.execute_input":"2022-06-06T14:48:29.976578Z","iopub.status.idle":"2022-06-06T14:48:30.518049Z","shell.execute_reply.started":"2022-06-06T14:48:29.976529Z","shell.execute_reply":"2022-06-06T14:48:30.517020Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (10,5)\n\ncorrMatrix = df.corr()\nsns.heatmap(corrMatrix, linewidths=1, cmap=\"YlGnBu\", annot=True)\nplt.show()\nprint(corrMatrix[\"Transported\"].abs().sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:30.519580Z","iopub.execute_input":"2022-06-06T14:48:30.520143Z","iopub.status.idle":"2022-06-06T14:48:31.073162Z","shell.execute_reply.started":"2022-06-06T14:48:30.520082Z","shell.execute_reply":"2022-06-06T14:48:31.072072Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Count the number of unique items in each columns:\ndf.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.074752Z","iopub.execute_input":"2022-06-06T14:48:31.075081Z","iopub.status.idle":"2022-06-06T14:48:31.105671Z","shell.execute_reply.started":"2022-06-06T14:48:31.075036Z","shell.execute_reply":"2022-06-06T14:48:31.104547Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.107226Z","iopub.execute_input":"2022-06-06T14:48:31.107597Z","iopub.status.idle":"2022-06-06T14:48:31.189846Z","shell.execute_reply.started":"2022-06-06T14:48:31.107545Z","shell.execute_reply":"2022-06-06T14:48:31.188842Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_how_many(df, feature):\n    '''\n    This function find how many observations of this feature was cancelled \n    input\n    feature: a column from the dataframe\n    output:\n    Feature: dataframe with the number of cancellations\n    '''\n    Feature = df[['Transported',feature]].groupby(['Transported',feature]).size().reset_index(name = 'count')\n    \n    return Feature","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.193734Z","iopub.execute_input":"2022-06-06T14:48:31.194030Z","iopub.status.idle":"2022-06-06T14:48:31.200659Z","shell.execute_reply.started":"2022-06-06T14:48:31.193996Z","shell.execute_reply":"2022-06-06T14:48:31.199512Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"count_how_many(df,'Age')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.202149Z","iopub.execute_input":"2022-06-06T14:48:31.202464Z","iopub.status.idle":"2022-06-06T14:48:31.234890Z","shell.execute_reply.started":"2022-06-06T14:48:31.202408Z","shell.execute_reply":"2022-06-06T14:48:31.233834Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"count = count_how_many(df, \"Age\")\ncount ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.236889Z","iopub.execute_input":"2022-06-06T14:48:31.237151Z","iopub.status.idle":"2022-06-06T14:48:31.259330Z","shell.execute_reply.started":"2022-06-06T14:48:31.237120Z","shell.execute_reply":"2022-06-06T14:48:31.256899Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\n# sns.set_theme(style=\"darkgrid\")\nsns.set_palette(\"pastel\")\n\n\nfig, axes = plt.subplots(1,2,figsize=(25,6))\n\nplot1 = sns.countplot(x=df['HomePlanet'],hue = df['Transported'], ax=axes[0])\naxes[0].set_title(\"Transported vs non-Transported base on HomePlanet group\", size=10)\n\nplot2 = sns.countplot(x=df['Destination'],hue = df['Transported'], ax=axes[1])\naxes[1].set_title(\"Transported vs non-Transported base on HomePlanet group\", size=10)\n\n\nplot1.bar_label(plot1.containers[0])\nplot1.bar_label(plot1.containers[1])\n\nplot2.bar_label(plot2.containers[0])\nplot2.bar_label(plot2.containers[1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.260739Z","iopub.execute_input":"2022-06-06T14:48:31.261283Z","iopub.status.idle":"2022-06-06T14:48:31.811337Z","shell.execute_reply.started":"2022-06-06T14:48:31.261240Z","shell.execute_reply":"2022-06-06T14:48:31.810473Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"- The bar plot on the **left** show that the majority of people who were transported come from Earth\n\n- The bar plot on the **right** show that passenger who travel to **Trappist-1e** are more likely to be transported to a different dimension compare to passengers whose destinations is either **PSO** or **55 Cabcri e**","metadata":{}},{"cell_type":"code","source":"# let's explore how people from different HomePlanet will tend to travel to the tree destinations ? \n\nfig, axes = plt.subplots(1,1,figsize=(20,5))\n\nsns.countplot(x=df['HomePlanet'], hue = df['Destination'], ax=axes)\naxes.set_title(\"Transported vs non-Transported base on HomePlanet group\", size=10)\n\n# We can see that the majority of people who comes from Earth travel to Trappist \n\n# From previous plots, we can safetly assume that people who travel to Trappist and are from Earth are the majority that get transported to \n# another dimension. ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:31.813033Z","iopub.execute_input":"2022-06-06T14:48:31.813566Z","iopub.status.idle":"2022-06-06T14:48:32.165793Z","shell.execute_reply.started":"2022-06-06T14:48:31.813516Z","shell.execute_reply":"2022-06-06T14:48:32.164710Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(20,5))\n\nsns.countplot(x=df['CryoSleep'], ax=axes[0])\naxes[0].set_title(\"Total that are in CryoSleep\", size=10)\n\nsns.countplot(x=df['CryoSleep'],hue = df['Transported'], ax=axes[1])\naxes[1].set_title(\"Total Transported vs non-Transported that are in CryoSleep\", size=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:32.167856Z","iopub.execute_input":"2022-06-06T14:48:32.168576Z","iopub.status.idle":"2022-06-06T14:48:32.593062Z","shell.execute_reply.started":"2022-06-06T14:48:32.168523Z","shell.execute_reply":"2022-06-06T14:48:32.592350Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from matplotlib.ticker import FormatStrFormatter\n\n\nplt.figure(figsize = (20,7))\ntrans_rate_age = df[['Transported', 'CryoSleep','Age']].groupby(\n   ['Transported','CryoSleep', 'Age']).size().reset_index(name ='count')\n\ntrans_rate_age = trans_rate_age.pivot_table(\n        index = ['CryoSleep', 'Age'], \n        columns ='Transported', \n       values = 'count').reset_index().rename(columns = {0:'not_Transported' ,1: 'Transported'})\ntrans_rate_age['trans_rate'] = trans_rate_age.Transported/ (trans_rate_age.Transported + trans_rate_age.not_Transported)\n\ntrans_rate_ax = sns.barplot(\n    x = 'Age', \n    hue ='CryoSleep',\n    y ='trans_rate',\n    palette = sns.color_palette(\"Set2\",2),\n    data = trans_rate_age\n)\n\nplt.title('Figure 3: Transported Rate by Age for those are in and not in Cryosleep')\nplt.xlabel('Age')\nplt.ylabel('Transported rate')\n\n\ntrans_rate_ax.set_xticklabels(trans_rate_ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:32.594865Z","iopub.execute_input":"2022-06-06T14:48:32.595536Z","iopub.status.idle":"2022-06-06T14:48:35.870566Z","shell.execute_reply.started":"2022-06-06T14:48:32.595485Z","shell.execute_reply":"2022-06-06T14:48:35.869903Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"- Looking at Figure 3, we can confidently conclude that the majority of people are in CryoSleep that are in the age range (14 - 61) get transported the most.\n\n- The rate of transportation for older people are less obvious but there are still some relatively high transportation rate for those in the age range 64 and 65.\n\n- For people that are 40 years old, 97% of them is transported while in Cryosleep.","metadata":{}},{"cell_type":"code","source":"trans_rate_age.head(30)\n# df.sort_values(by=['Age'])\ndf['Age'].value_counts().head(40)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:35.871955Z","iopub.execute_input":"2022-06-06T14:48:35.872405Z","iopub.status.idle":"2022-06-06T14:48:35.886178Z","shell.execute_reply.started":"2022-06-06T14:48:35.872371Z","shell.execute_reply":"2022-06-06T14:48:35.884797Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trans_rate_age","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:35.888105Z","iopub.execute_input":"2022-06-06T14:48:35.888498Z","iopub.status.idle":"2022-06-06T14:48:35.919764Z","shell.execute_reply.started":"2022-06-06T14:48:35.888433Z","shell.execute_reply":"2022-06-06T14:48:35.918794Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n\nyoung = df[df[\"Age\"] <= 18]\nyoung_adult = df[(df[\"Age\"] > 18) & (df[\"Age\"] <= 40)]\nadolescent = df[(df[\"Age\"] > 40) & (df[\"Age\"] <= 60)]\nsenior = df[df[\"Age\"] > 60]\n\nfig, axes = plt.subplots(1,3,figsize=(21,6))\n\nsns.countplot(x=young['Age'],hue = young['Transported'], ax=axes[0])\n\nsns.countplot(x=young_adult['Age'],hue = young_adult['Transported'], ax=axes[1])\n\nsns.countplot(x=adolescent['Age'],hue = adolescent['Transported'], ax=axes[2])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:35.921129Z","iopub.execute_input":"2022-06-06T14:48:35.921400Z","iopub.status.idle":"2022-06-06T14:48:37.464843Z","shell.execute_reply.started":"2022-06-06T14:48:35.921368Z","shell.execute_reply":"2022-06-06T14:48:37.463770Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# This plot demonstrate the distribution of the Age of the Passengers. Note that this includes missing values. \n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.distplot(x=df.Age, bins = 50, color=\"purple\")\n\nax.set_xticks(range(-2,100,2))\nax.set(xlabel='Passenger Age')\n\nplt.show()\n\n# Young people from the age of roughly 15 to 35 are among the most likely to be transported.","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:37.466165Z","iopub.execute_input":"2022-06-06T14:48:37.466409Z","iopub.status.idle":"2022-06-06T14:48:38.741211Z","shell.execute_reply.started":"2022-06-06T14:48:37.466380Z","shell.execute_reply":"2022-06-06T14:48:38.740302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import scipy\n\n\nstat, p = scipy.stats.shapiro(df.Age)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpretation\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:38.743023Z","iopub.execute_input":"2022-06-06T14:48:38.743676Z","iopub.status.idle":"2022-06-06T14:48:38.755753Z","shell.execute_reply.started":"2022-06-06T14:48:38.743624Z","shell.execute_reply":"2022-06-06T14:48:38.754657Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,1,figsize=(25,10))\n\nsns.countplot(y=df['Transported'],hue = df['Age']).set_title(\"Transported vs non-Transported base on HomePlanet group\", size=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:38.757269Z","iopub.execute_input":"2022-06-06T14:48:38.758097Z","iopub.status.idle":"2022-06-06T14:48:40.837303Z","shell.execute_reply.started":"2022-06-06T14:48:38.757850Z","shell.execute_reply":"2022-06-06T14:48:40.836531Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,1,figsize=(10,8))\n\n\nsns.boxplot(x='Transported',y='Age', data=df,showfliers=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:40.838558Z","iopub.execute_input":"2022-06-06T14:48:40.839413Z","iopub.status.idle":"2022-06-06T14:48:41.251726Z","shell.execute_reply.started":"2022-06-06T14:48:40.839374Z","shell.execute_reply":"2022-06-06T14:48:41.250811Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### TEXT ANALYSIS OF PASSENGER ID","metadata":{}},{"cell_type":"code","source":"# The end number of the ID indicate whether someone is in a group. We want to know if people who are single tend to disappear more\n# than people who travel in groups\n\nID = df[\"PassengerId\"]\nID.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:41.253085Z","iopub.execute_input":"2022-06-06T14:48:41.253346Z","iopub.status.idle":"2022-06-06T14:48:41.262111Z","shell.execute_reply.started":"2022-06-06T14:48:41.253312Z","shell.execute_reply":"2022-06-06T14:48:41.261010Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"ID.str.len()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:41.263866Z","iopub.execute_input":"2022-06-06T14:48:41.264790Z","iopub.status.idle":"2022-06-06T14:48:41.288354Z","shell.execute_reply.started":"2022-06-06T14:48:41.264740Z","shell.execute_reply":"2022-06-06T14:48:41.287362Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# TRY THINGS OUT\n\nsingle = ID.str.contains('_01')\ngroup = ~ID.str.contains('_01')  \n\ngroup","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:41.289910Z","iopub.execute_input":"2022-06-06T14:48:41.290785Z","iopub.status.idle":"2022-06-06T14:48:41.313018Z","shell.execute_reply.started":"2022-06-06T14:48:41.290704Z","shell.execute_reply":"2022-06-06T14:48:41.312076Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndf2 = df.copy()\n\ndf2['PassengerType'] = np.where(ID.str.contains('_01'), True, False)\ndf2['PassengerType'] = df2['PassengerType'].map({True: 'Single', False: 'Groups'})\n\n\n\ncabin_side = df[\"Cabin\"]\ndf2['CabinSide'] = np.where(cabin_side.str.contains('/P'), True, False)\n\ndf2['CabinSide'] = df2['CabinSide'].map({True: 'Port', False: 'Starboard'})\n\ndf2.dtypes\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:41.314751Z","iopub.execute_input":"2022-06-06T14:48:41.315286Z","iopub.status.idle":"2022-06-06T14:48:41.351650Z","shell.execute_reply.started":"2022-06-06T14:48:41.315234Z","shell.execute_reply":"2022-06-06T14:48:41.350709Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"for dataset in df2:\n    df2['Deck'] = df2['Cabin'].apply(lambda x: x.split('/')[0] if (str(x)) != 'nan' else x)\n    \n\nfig, axes = plt.subplots(1,2,figsize=(15,5))\n\nsns.countplot(data = df2, x = \"Deck\", hue = \"Transported\", ax=axes[0])\nsns.lineplot(x = \"Deck\", y = \"Transported\", data = df2, ci=None, linewidth=3, marker=\"o\", ax=axes[1])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:41.360179Z","iopub.execute_input":"2022-06-06T14:48:41.361348Z","iopub.status.idle":"2022-06-06T14:48:42.038175Z","shell.execute_reply.started":"2022-06-06T14:48:41.361281Z","shell.execute_reply":"2022-06-06T14:48:42.037071Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df2[\"Transported\"] = df[\"Transported\"].astype(int)\n\nax1 = df2.plot.scatter(x='Spa',\n                      y= 'Transported',\n                     c='DarkBlue')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:42.040206Z","iopub.execute_input":"2022-06-06T14:48:42.040587Z","iopub.status.idle":"2022-06-06T14:48:42.391230Z","shell.execute_reply.started":"2022-06-06T14:48:42.040538Z","shell.execute_reply":"2022-06-06T14:48:42.389973Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3,figsize=(25,6))\n\nplot4 = sns.countplot(x=df2['PassengerType'], hue = df2['Transported'], ax=axes[0], palette = sns.color_palette(\"Set1\",2))\naxes[0].set_title(\"Transported vs non-Transported base on Passenger Type (single or groups)\", size=10)\n\nplot5 = sns.countplot(x=df2['CabinSide'], hue = df2['Transported'], ax=axes[1], palette = sns.color_palette(\"Set3\",2))\naxes[1].set_title(\"Transported vs non-Transported base on Cabin Side\", size=10)\n\nplot6 = sns.countplot(x=df2['VIP'], hue = df2['Transported'], ax=axes[2], palette = sns.color_palette(\"Set2\",2))\naxes[2].set_title(\"Transported vs non-Transported base on VIP status\", size=10)\n\nplot4.bar_label(plot4.containers[0])\nplot4.bar_label(plot4.containers[1])\n\nplot5.bar_label(plot5.containers[0])\nplot5.bar_label(plot5.containers[1])\n\nplot6.bar_label(plot6.containers[0])\nplot6.bar_label(plot6.containers[1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:42.392784Z","iopub.execute_input":"2022-06-06T14:48:42.393044Z","iopub.status.idle":"2022-06-06T14:48:43.032416Z","shell.execute_reply.started":"2022-06-06T14:48:42.393011Z","shell.execute_reply":"2022-06-06T14:48:43.031384Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"**False** = not single \n\n**True** = single passenger\n\nThe plot above show that the number of people was transported in the single group were twice as many compare to people who travel in groups\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25,7))\ntrans_rate_age2 = df2[['Transported', 'PassengerType','Age']].groupby(\n   ['Transported','PassengerType', 'Age']).size().reset_index(name ='count')\n\ntrans_rate_age2 = trans_rate_age2.pivot_table(\n        index = ['PassengerType', 'Age'], \n        columns ='Transported', \n       values = 'count').reset_index().rename(columns = {0:'not_Transported' ,1: 'Transported'}).fillna(0)\ntrans_rate_age2['trans_rate2'] = trans_rate_age2.Transported / (trans_rate_age2.Transported + trans_rate_age2.not_Transported)\n\ntrans_rate_ax2 = sns.barplot(\n    x = 'Age', \n    hue ='PassengerType',\n    y ='trans_rate2',\n    palette = sns.color_palette(\"Set2\",2),\n    data = trans_rate_age2\n)\n\nplt.title('Figure 4: Transported Rate by Age base on PassengerType')\nplt.xlabel('Age')\nplt.ylabel('Transported rate')\n\n\ntrans_rate_ax2.set_xticklabels(trans_rate_ax2.get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\n# plt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:43.033973Z","iopub.execute_input":"2022-06-06T14:48:43.034365Z","iopub.status.idle":"2022-06-06T14:48:45.850625Z","shell.execute_reply.started":"2022-06-06T14:48:43.034323Z","shell.execute_reply":"2022-06-06T14:48:45.849429Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\ntrans_rate_age3 = df[['Transported', 'HomePlanet','Destination']].groupby(\n   ['Transported','HomePlanet', 'Destination']).size().reset_index(name ='count')\n\ntrans_rate_age3 = trans_rate_age3.pivot_table(\n        index = ['HomePlanet', 'Destination'], \n        columns ='Transported', \n       values = 'count').reset_index().rename(columns = {0:'not_Transported' ,1: 'Transported'}).fillna(0)\n\ntrans_rate_age3['trans_rate3'] =  trans_rate_age3.Transported\n\ntrans_rate_ax3 = sns.barplot(\n    x = 'HomePlanet', \n    hue = 'Destination',\n    y = 'trans_rate3',\n    palette = sns.color_palette(\"husl\", 4),\n    data = trans_rate_age3\n)\n\n\ntrans_rate_ax3.bar_label(trans_rate_ax3.containers[0])\ntrans_rate_ax3.bar_label(trans_rate_ax3.containers[1])\ntrans_rate_ax3.bar_label(trans_rate_ax3.containers[2])\n\nplt.title('Figure 5: Number of Transported base on HomePlanet and Destination')\nplt.xlabel('HomePlanet')\nplt.ylabel('Number of Transported')\n\n\nplt.tight_layout()\n\n\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:45.852124Z","iopub.execute_input":"2022-06-06T14:48:45.852486Z","iopub.status.idle":"2022-06-06T14:48:46.274510Z","shell.execute_reply.started":"2022-06-06T14:48:45.852424Z","shell.execute_reply":"2022-06-06T14:48:46.273622Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"From this, we can conclude that people who travel from **Earth** to **Trappist** make up the majority of the people who disappeared.\n","metadata":{}},{"cell_type":"code","source":"trans_rate_age3['trans_rate3']","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:46.275951Z","iopub.execute_input":"2022-06-06T14:48:46.276198Z","iopub.status.idle":"2022-06-06T14:48:46.286169Z","shell.execute_reply.started":"2022-06-06T14:48:46.276169Z","shell.execute_reply":"2022-06-06T14:48:46.284698Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for columns with missing values and calculating the percentages\ndata = {'Missing value count': df.isnull().sum(), 'Percentage (%)': (df.isnull().sum()/df.shape[0]) * 100}\ndf_missing = pd.DataFrame(data=data)\nprint(df_missing[df_missing['Missing value count'] > 0])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:46.287797Z","iopub.execute_input":"2022-06-06T14:48:46.288597Z","iopub.status.idle":"2022-06-06T14:48:46.320955Z","shell.execute_reply.started":"2022-06-06T14:48:46.288552Z","shell.execute_reply":"2022-06-06T14:48:46.320194Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\nmsno.bar(df)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:46.322194Z","iopub.execute_input":"2022-06-06T14:48:46.322558Z","iopub.status.idle":"2022-06-06T14:48:47.378648Z","shell.execute_reply.started":"2022-06-06T14:48:46.322527Z","shell.execute_reply":"2022-06-06T14:48:47.377938Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"msno.matrix(df)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:47.379813Z","iopub.execute_input":"2022-06-06T14:48:47.380177Z","iopub.status.idle":"2022-06-06T14:48:47.991233Z","shell.execute_reply.started":"2022-06-06T14:48:47.380145Z","shell.execute_reply":"2022-06-06T14:48:47.990218Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# df2.duplicated(subset=None, keep='first')\n\nduplicate = df2[df2.duplicated()]\n \nprint(\"Duplicate Rows :\")\n \n# Print the resultant Dataframe\nduplicate","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:47.993328Z","iopub.execute_input":"2022-06-06T14:48:47.993884Z","iopub.status.idle":"2022-06-06T14:48:48.026356Z","shell.execute_reply.started":"2022-06-06T14:48:47.993832Z","shell.execute_reply":"2022-06-06T14:48:48.025726Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### SPLIT HERE","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n\n# Assigning numerical values and storing in another column\ndf2['Transported'] = labelencoder.fit_transform(df2['Transported'])\n\n\n# # Creating a 60 - 40 split between training and holdout set\n# train_set, holdout = train_test_split(df_clean, test_size=0.4, random_state=1234)\n\n# # Creatining 50 - 50 split between validation and test set\n# val_set, test_set = train_test_split(holdout, test_size=0.5, random_state=1234)\n\n\n# drop unwanter columns \n\n\nX = df2.drop(columns = ['Transported'])\ny = df2.Transported\n\n# Split the test data columns into output and features \nX_test = test_df.drop(columns = ['Transported'])\ny_test = test_df.Transported\n\n# Split the training data further into 60-20: 60 training and 20 validations + 20 test data from before\nX_train, X_val, y_train, y_val = train_test_split(X,y, # data to split\n                                                  test_size = 0.25,    # we will leave 25% to validate our models\n                                                  random_state = 1234,  # make our work reproducable \n                                                  shuffle = True)     # prevent data ordering affecting our model\n\n\n\n\n# reset the index so it goes from 0...n\nX_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\nX_val = X_val.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)\n\n\n# # The hold-out set won't be touched until model selection.\n\n# # Creating a 60 - 40 splits between train and hold-out sets\n# X_train, X_hold_out, y_train, y_hold_out = train_test_split(X, y, test_size=0.4, random_state=1234)\n\n# #Split the hold-out 50 - 50 into validation and test set \n# X_val, X_test, y_val, y_test = train_test_split(X_hold_out ,y_hold_out ,test_size=0.5, random_state=1234) # For model selection\n\n# We verify that the training, validation and test set are split accordingly.\nprint(len(X_train), 'train samples')\nprint(len(X_val), 'validation samples')\nprint(len(test_df), 'test samples')\nprint(f'Percentage of train samples: {round(len(X_train) / len(df0)*100)}%')\nprint(f'Percentage of validation samples: {round(len(X_val) / len(df0)*100)}%')\nprint(f'Percentage of test samples: {round(len(test_df) / len(df0)*100)}%')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.027699Z","iopub.execute_input":"2022-06-06T14:48:48.028145Z","iopub.status.idle":"2022-06-06T14:48:48.051646Z","shell.execute_reply.started":"2022-06-06T14:48:48.028109Z","shell.execute_reply":"2022-06-06T14:48:48.050700Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### FEATURE ENGINERRING","metadata":{}},{"cell_type":"code","source":"def impute_values(df):\n\n    '''\n    This function cleans the NA values present in a dataframe \n    If the column has numerical values, then it will fill the NA's with the median value.\n    Else, it will fill the NA's with the mode value (categorical variables)\n    input:\n    df: pandas dataframe to clean\n    output:\n    df: cleaned pandas dataframe\n    '''\n    # Get the column names with NA values\n    cols_na = [col for col in df.columns if df[col].isnull().any()]\n\n    # Store the values to use for filling the NA's in a temporary dictionary\n    values_dict = {}\n    for i in cols_na:\n\n        if df[i].dtype == \"float64\":\n            clean_value = df[i].median()\n\n        else:\n            clean_value = df[i].mode()[0]\n            \n        values_dict[i] = clean_value\n    \n    # Replace the NA's with the dictionary\n    df.fillna(value = values_dict, inplace = True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.052923Z","iopub.execute_input":"2022-06-06T14:48:48.053272Z","iopub.status.idle":"2022-06-06T14:48:48.061407Z","shell.execute_reply.started":"2022-06-06T14:48:48.053237Z","shell.execute_reply":"2022-06-06T14:48:48.060484Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# IMPUTE MISSING DATA\n\nX_train = impute_values(X_train)\n\nX_val = impute_values(X_val)\n\n# X_test = impute_values(X_test)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.063521Z","iopub.execute_input":"2022-06-06T14:48:48.063939Z","iopub.status.idle":"2022-06-06T14:48:48.132841Z","shell.execute_reply.started":"2022-06-06T14:48:48.063888Z","shell.execute_reply":"2022-06-06T14:48:48.131719Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"data = {'Missing value count': X_train.isnull().sum(), 'Percentage (%)': (X_train.isnull().sum()/X_train.shape[0]) * 100}\ndf_missing = pd.DataFrame(data=data)\nprint(df_missing[df_missing['Missing value count'] >= 0])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.134332Z","iopub.execute_input":"2022-06-06T14:48:48.134585Z","iopub.status.idle":"2022-06-06T14:48:48.160477Z","shell.execute_reply.started":"2022-06-06T14:48:48.134556Z","shell.execute_reply":"2022-06-06T14:48:48.159392Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# NOTE THAT df_clean ONLY CONTAIN THE TRAINING DATASET WITH THE TARGET COLUMN THAT HAS ALREADY BEEN DROPPED.\n\nX_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.162128Z","iopub.execute_input":"2022-06-06T14:48:48.162517Z","iopub.status.idle":"2022-06-06T14:48:48.170884Z","shell.execute_reply.started":"2022-06-06T14:48:48.162472Z","shell.execute_reply":"2022-06-06T14:48:48.169816Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"header = \"Dataset information\"\nprint(header)\nprint(\"=\"*len(header))\n\n# Printing data set dimensions\nprint(\"Number of rows: \", X_train.shape[0])\nprint(\"Number of columns: \", X_train.shape[1])\nprint(\"Column names:\", )\nfor i in X_train.columns:\n    print(i, end = \",\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.172280Z","iopub.execute_input":"2022-06-06T14:48:48.172573Z","iopub.status.idle":"2022-06-06T14:48:48.190927Z","shell.execute_reply.started":"2022-06-06T14:48:48.172539Z","shell.execute_reply":"2022-06-06T14:48:48.190022Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# DIMENSIONALITY REDUCTION","metadata":{}},{"cell_type":"code","source":"# TESTING PCA\n\n\nfeatures = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\nvar = X_train.copy()\n\nvar = var.loc[:, features]\n\n# Standardize\nvar_scaled = (var - var.mean(axis=0)) / var.std(axis=0)\n\nfrom sklearn.decomposition import PCA\n\n# Create principal components\npca = PCA()\nvar_pca = pca.fit_transform(var_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(var_pca.shape[1])]\nvar_pca = pd.DataFrame(var_pca, columns=component_names)\n\nlen(var_pca.index)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.192319Z","iopub.execute_input":"2022-06-06T14:48:48.192620Z","iopub.status.idle":"2022-06-06T14:48:48.332111Z","shell.execute_reply.started":"2022-06-06T14:48:48.192579Z","shell.execute_reply":"2022-06-06T14:48:48.331199Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=var.columns,  # and the rows are the original features\n)\nloadings","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.333472Z","iopub.execute_input":"2022-06-06T14:48:48.334326Z","iopub.status.idle":"2022-06-06T14:48:48.349990Z","shell.execute_reply.started":"2022-06-06T14:48:48.334278Z","shell.execute_reply":"2022-06-06T14:48:48.349065Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.351603Z","iopub.execute_input":"2022-06-06T14:48:48.352122Z","iopub.status.idle":"2022-06-06T14:48:48.782665Z","shell.execute_reply.started":"2022-06-06T14:48:48.352073Z","shell.execute_reply":"2022-06-06T14:48:48.781896Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"sources: \n\nhttps://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a\n\nhttps://www.kaggle.com/code/nirajvermafcb/principal-component-analysis-explained/notebook\n\nhttps://www.kaggle.com/code/ryanholbrook/principal-component-analysis/tutorial","metadata":{}},{"cell_type":"markdown","source":"## MUTUAL INFORMATION","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\nplt.figure(figsize=(15,10))\n\nmi = mutual_info_classif(var_scaled, y_train)\nmi_series = pd.Series(mi).sort_values(ascending = False)\n\n# lets plot the top 30\nPLOT_NUM = 30\nmi_series[:PLOT_NUM].plot.bar(legend = False, figsize=(15,10))\nplt.title('Mutual information (' + str(PLOT_NUM) + ')')\n\nplt.xticks(rotation=45,ha='right')\nplt.tight_layout()\n#plt.savefig('mutual_information.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:48.783843Z","iopub.execute_input":"2022-06-06T14:48:48.784206Z","iopub.status.idle":"2022-06-06T14:48:49.184473Z","shell.execute_reply.started":"2022-06-06T14:48:48.784176Z","shell.execute_reply":"2022-06-06T14:48:49.183513Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nRANDOM_STATE = 1234\n\n# create a forest classifier\nforest = RandomForestClassifier(criterion='gini',\n                                n_estimators=1000,\n                                max_features = 'sqrt',\n                                random_state=RANDOM_STATE,\n                                n_jobs=-1)\n\n# fit the classifier\nforest.fit(var_scaled, y_train)\n\n# get the importances for the features\nimportances = forest.feature_importances_\n\nimportances_series = pd.Series(importances).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:49.185917Z","iopub.execute_input":"2022-06-06T14:48:49.186242Z","iopub.status.idle":"2022-06-06T14:48:54.183341Z","shell.execute_reply.started":"2022-06-06T14:48:49.186203Z","shell.execute_reply":"2022-06-06T14:48:54.182307Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"PLOT_LIMIT = 12\n\n# plot the important features\nplt.figure(figsize=(15,10))\nimportances_series[:PLOT_LIMIT].plot.bar(legend =False, grid=False)\nplt.title('Feature Importances ('+str(PLOT_LIMIT)+'/'+str(len(importances_series))+')')\n\nplt.xticks(rotation=45,ha='right')\nplt.tight_layout()\n\nplt.savefig('forest_importances.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:54.184856Z","iopub.execute_input":"2022-06-06T14:48:54.185189Z","iopub.status.idle":"2022-06-06T14:48:55.168421Z","shell.execute_reply.started":"2022-06-06T14:48:54.185143Z","shell.execute_reply":"2022-06-06T14:48:55.167532Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"df3 = X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.170069Z","iopub.execute_input":"2022-06-06T14:48:55.170607Z","iopub.status.idle":"2022-06-06T14:48:55.175049Z","shell.execute_reply.started":"2022-06-06T14:48:55.170559Z","shell.execute_reply":"2022-06-06T14:48:55.174135Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Feature transformation and drop ID, Name and Cabin","metadata":{}},{"cell_type":"code","source":"# One hot encode cat features\ncat_feat=[\"HomePlanet\", \"Destination\", \"Deck\"]\n\nX_train = pd.get_dummies(X_train, columns=cat_feat, drop_first = True)\n\nX_val = pd.get_dummies(X_val, columns=cat_feat, drop_first = True)\n\n\n\n# Feature transformation \nX_train['PassengerType'] = X_train['PassengerType'].replace(['Single', 'Groups'], [1, 0])\nX_train['CabinSide'] = X_train['CabinSide'].replace(['Port', 'Starboard'], [1, 0])\n\n# X_train['HomePlanet'] = X_train['HomePlanet'].replace(['Earth', 'Europa', 'Mars'], [2, 1, 0])\n# X_train['Destination'] = X_train['Destination'].replace(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], [2, 1, 0])\n\n# Drop Features that have already been transformed into integer\nX_train = X_train.drop(columns = [\"PassengerId\", \"Cabin\", \"Name\"])\n\n\n######################################################################\n\nX_val['PassengerType'] = X_val['PassengerType'].replace(['Single', 'Groups'], [1, 0])\nX_val['CabinSide'] = X_val['CabinSide'].replace(['Port', 'Starboard'], [1, 0])\n\n# X_val['HomePlanet'] = X_val['HomePlanet'].replace(['Earth', 'Europa', 'Mars'], [2, 1, 0])\n# X_val['Destination'] = X_val['Destination'].replace(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], [2, 1, 0])\n\n# Drop Features that have already been transformed into integer\nX_val = X_val.drop(columns = [\"PassengerId\", \"Cabin\", \"Name\"])\n\n###################################################################################################\n\n# X_test['PassengerType'] = X_test['PassengerType'].replace(['Single', 'Groups'], [1, 0])\n# X_test['CabinSide'] = X_test['CabinSide'].replace(['Port', 'Starboard'], [1, 0])\n\n# X_test['HomePlanet'] = X_test['HomePlanet'].replace(['Earth', 'Europa', 'Mars'], [2, 1, 0])\n# X_test['Destination'] = X_test['Destination'].replace(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], [2, 1, 0])\n\n# #Drop Features that have already been transformed into integer\n# X_test = X_test.drop(columns = [\"PassengerId\", \"Cabin\", \"Name\"])\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.176550Z","iopub.execute_input":"2022-06-06T14:48:55.176809Z","iopub.status.idle":"2022-06-06T14:48:55.225002Z","shell.execute_reply.started":"2022-06-06T14:48:55.176776Z","shell.execute_reply":"2022-06-06T14:48:55.224087Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#Convert some columns to int because machine learning dont take in BOolean values\n\nX_train[['CryoSleep', 'VIP']] = (X_train[['CryoSleep', 'VIP']] == 'TRUE').astype(int)\n\nX_val[['CryoSleep', 'VIP']] = (X_val[['CryoSleep', 'VIP']] == 'TRUE').astype(int)\n\n# X_test[['CryoSleep', 'VIP']] = (X_test[['CryoSleep', 'VIP']] == 'TRUE').astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.226773Z","iopub.execute_input":"2022-06-06T14:48:55.227087Z","iopub.status.idle":"2022-06-06T14:48:55.238097Z","shell.execute_reply.started":"2022-06-06T14:48:55.227042Z","shell.execute_reply":"2022-06-06T14:48:55.236714Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# MODEL PERFORMANCE\n\nimport sklearn\n\ndef roc_plot(y_true, y_pred):\n    \"\"\" Draw an ROC curve and report AUC\n    \"\"\"\n    roc = pd.DataFrame(\n        data = np.c_[sklearn.metrics.roc_curve(y_true, y_pred)],\n        columns = ('fpr', 'tpr', 'threshold')\n    )\n    \n    sns.lineplot(x='fpr', y='tpr', data=roc, ci=None)\n\n\n    plt.plot([0,1],[0,1], 'k--', alpha=0.5) # 0-1 line \n    plt.title(\"ROC curve (auc = %.4f)\" % sklearn.metrics.roc_auc_score(y_true, y_pred))\n             \n    plt.show()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n#Function returns confusion matrix and classification report for the model\ndef assess_model(actual, pred):\n    print(\"Confusion Matrix:\\n\", confusion_matrix(actual, pred), \"\\n\")\n    print(\"Classification Report:\\n\", classification_report(actual, pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.240129Z","iopub.execute_input":"2022-06-06T14:48:55.240554Z","iopub.status.idle":"2022-06-06T14:48:55.256325Z","shell.execute_reply.started":"2022-06-06T14:48:55.240477Z","shell.execute_reply":"2022-06-06T14:48:55.255226Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### TESTING DATA HERE","metadata":{}},{"cell_type":"code","source":"# #Cleaning up TEST data (similar to the train data) ready for model \n\n# df_test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n\n# df_test_copy = df_test.copy()\n\n\n# ID = df_test_copy[\"PassengerId\"]\n# df_test_copy['PassengerType'] = np.where(ID.str.contains('_01'), True, False)\n\n\n# cabin_side = df_test_copy [\"Cabin\"]\n# df_test_copy['CabinSide'] = np.where(cabin_side.str.contains('/P'), True, False)\n\n\n# df_test_copy['HomePlanet'] = df_test_copy['HomePlanet'].replace(['Earth', 'Europa', 'Mars'], [2, 1, 0])\n# df_test_copy['Destination'] = df_test_copy['Destination'].replace(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], [2, 1, 0])\n\n\n# # Drop Features that have already been transformed into integer\n# df_test_drop = df_test_copy.drop(columns = [\"PassengerId\", \"Cabin\", \"Name\"])\n\n\n# df_test_clean = impute_values(df_test_drop)\n\n\n# df_test_clean.dtypes\n\n# #CHECK FOR MISSING VALUES\n# percent_missing = df_test_clean.isnull().sum() * 100 / len(df_test_clean)\n# missing_value_df = pd.DataFrame({'column_name': df_test_clean.columns,\n#                                  'percent_missing': percent_missing})\n\n# percent_missing\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.258189Z","iopub.execute_input":"2022-06-06T14:48:55.258561Z","iopub.status.idle":"2022-06-06T14:48:55.271096Z","shell.execute_reply.started":"2022-06-06T14:48:55.258515Z","shell.execute_reply":"2022-06-06T14:48:55.269801Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"#baseline model \nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nm_baseline = LogisticRegression(solver='lbfgs', max_iter=2000).fit(X_train, y_train)\nassess_model(y_val, m_baseline.predict(X_val))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:55.272828Z","iopub.execute_input":"2022-06-06T14:48:55.273296Z","iopub.status.idle":"2022-06-06T14:48:56.134149Z","shell.execute_reply.started":"2022-06-06T14:48:55.273250Z","shell.execute_reply":"2022-06-06T14:48:56.133007Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Hyper parameter tuning:\n\n# Increase max iter to 4000 resolve the LineSearch Warning errors.\n\n# Dont have to parameter tuning until you have found a model with the highest accuracy. In the future, if you found a non-tune model \n# that can perform better than your current well tuned one then you have wasted computational time \n# and efforts for the model that you will never use. \n\nmodel = LogisticRegression(max_iter = 4000)\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01] # this is the lambda penalty term in the objective function.\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state= 1234)\n\ngrid_search = GridSearchCV(estimator = model, param_grid = grid, n_jobs = -1, cv = cv, scoring = 'accuracy',error_score = 0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:48:56.140098Z","iopub.execute_input":"2022-06-06T14:48:56.141005Z","iopub.status.idle":"2022-06-06T14:49:43.514830Z","shell.execute_reply.started":"2022-06-06T14:48:56.140939Z","shell.execute_reply":"2022-06-06T14:49:43.513707Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"m_baseline = LogisticRegression(penalty='l2', C = 1.0, solver='liblinear', max_iter = 4000).fit(X_train, y_train)\n\nassess_model(y_val, m_baseline.predict(X_val))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.517266Z","iopub.execute_input":"2022-06-06T14:49:43.517899Z","iopub.status.idle":"2022-06-06T14:49:43.606301Z","shell.execute_reply.started":"2022-06-06T14:49:43.517840Z","shell.execute_reply":"2022-06-06T14:49:43.605290Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize = (10,5))\n\n# roc_plot(y_test, m_baseline.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.608527Z","iopub.execute_input":"2022-06-06T14:49:43.609281Z","iopub.status.idle":"2022-06-06T14:49:43.613699Z","shell.execute_reply.started":"2022-06-06T14:49:43.609226Z","shell.execute_reply":"2022-06-06T14:49:43.612809Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.615640Z","iopub.execute_input":"2022-06-06T14:49:43.616333Z","iopub.status.idle":"2022-06-06T14:49:43.676484Z","shell.execute_reply.started":"2022-06-06T14:49:43.616280Z","shell.execute_reply":"2022-06-06T14:49:43.675493Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### NAIVE BAYES CLASSIFIER ","metadata":{}},{"cell_type":"markdown","source":"Note: Naive Bayes assume independnt among the features. If you choose NB as your algorithms, make sure to check for the independent hypothesis","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train_NB = scaler.fit_transform(X_train)\n\nX_val_NB = scaler.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.678596Z","iopub.execute_input":"2022-06-06T14:49:43.679321Z","iopub.status.idle":"2022-06-06T14:49:43.710212Z","shell.execute_reply.started":"2022-06-06T14:49:43.679268Z","shell.execute_reply":"2022-06-06T14:49:43.709420Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_train_NB, y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_val_NB)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.711622Z","iopub.execute_input":"2022-06-06T14:49:43.712077Z","iopub.status.idle":"2022-06-06T14:49:43.725059Z","shell.execute_reply.started":"2022-06-06T14:49:43.712032Z","shell.execute_reply":"2022-06-06T14:49:43.724013Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(gnb.score(X_train_NB, y_train)))\n\nprint('Validation set score: {:.4f}'.format(gnb.score(X_val_NB, y_val)))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.726347Z","iopub.execute_input":"2022-06-06T14:49:43.726642Z","iopub.status.idle":"2022-06-06T14:49:43.737115Z","shell.execute_reply.started":"2022-06-06T14:49:43.726608Z","shell.execute_reply":"2022-06-06T14:49:43.736208Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(y_val, y_pred)\nac = accuracy_score(y_val,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.738499Z","iopub.execute_input":"2022-06-06T14:49:43.739167Z","iopub.status.idle":"2022-06-06T14:49:43.752712Z","shell.execute_reply.started":"2022-06-06T14:49:43.739130Z","shell.execute_reply":"2022-06-06T14:49:43.751757Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# visualize confusion matrix with seaborn heatmap\n\nplt.figure(figsize = (7,5))\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:43.754094Z","iopub.execute_input":"2022-06-06T14:49:43.754600Z","iopub.status.idle":"2022-06-06T14:49:44.035312Z","shell.execute_reply.started":"2022-06-06T14:49:43.754565Z","shell.execute_reply":"2022-06-06T14:49:44.034667Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:44.036668Z","iopub.execute_input":"2022-06-06T14:49:44.037083Z","iopub.status.idle":"2022-06-06T14:49:44.049243Z","shell.execute_reply.started":"2022-06-06T14:49:44.037050Z","shell.execute_reply":"2022-06-06T14:49:44.048209Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RANDOM FOREST MODEL","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Grid search for Random forest using max_depth, n_estimators and max_features\nrf_gs = GridSearchCV(\n    make_pipeline(\n        StandardScaler(), \n        RandomForestClassifier(criterion=\"gini\", bootstrap=True)\n        ),\n    param_grid={\n        \"randomforestclassifier__max_depth\": [5, 10, 20],\n        \"randomforestclassifier__n_estimators\": [10, 20, 50, 100],\n        \"randomforestclassifier__max_features\": [2, 5, 10, 15, 18]},\n    cv=KFold(5, shuffle=True, random_state=1234),\n    scoring=\"accuracy\"\n    ).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:49:44.050534Z","iopub.execute_input":"2022-06-06T14:49:44.050801Z","iopub.status.idle":"2022-06-06T14:51:14.511219Z","shell.execute_reply.started":"2022-06-06T14:49:44.050770Z","shell.execute_reply":"2022-06-06T14:51:14.510181Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"rf_gs.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:14.512604Z","iopub.execute_input":"2022-06-06T14:51:14.512943Z","iopub.status.idle":"2022-06-06T14:51:14.518853Z","shell.execute_reply.started":"2022-06-06T14:51:14.512906Z","shell.execute_reply":"2022-06-06T14:51:14.518136Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"rf_mdl = make_pipeline(\n        StandardScaler(), \n        RandomForestClassifier(\n            n_estimators = 100,\n            max_depth = 10, \n            max_features = 5 )\n        ).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:14.520140Z","iopub.execute_input":"2022-06-06T14:51:14.520480Z","iopub.status.idle":"2022-06-06T14:51:15.064380Z","shell.execute_reply.started":"2022-06-06T14:51:14.520431Z","shell.execute_reply":"2022-06-06T14:51:15.063727Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, plot_confusion_matrix\n\n\ndef model_assessment(model, X, y):\n\n    cm = plot_confusion_matrix(model, X, y, display_labels = [\"not_Transported\", \"Transported\"], cmap = plt.cm.Blues, normalize = \"true\")\n    y_pred = model.predict(X)\n    print(\"Accuracy score: \", accuracy_score(y, y_pred))\n\n    return cm","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:15.065594Z","iopub.execute_input":"2022-06-06T14:51:15.066406Z","iopub.status.idle":"2022-06-06T14:51:15.072654Z","shell.execute_reply.started":"2022-06-06T14:51:15.066352Z","shell.execute_reply":"2022-06-06T14:51:15.071874Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"model_assessment(rf_mdl, X_val, y_val)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:15.073677Z","iopub.execute_input":"2022-06-06T14:51:15.074355Z","iopub.status.idle":"2022-06-06T14:51:15.409052Z","shell.execute_reply.started":"2022-06-06T14:51:15.074316Z","shell.execute_reply":"2022-06-06T14:51:15.408143Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nm_rf = RandomForestClassifier(criterion=\"gini\")\nm_rf_fit = m_rf.fit(X_train,y_train)\nassess_model(y_val, m_rf_fit.predict(X_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:15.410689Z","iopub.execute_input":"2022-06-06T14:51:15.411152Z","iopub.status.idle":"2022-06-06T14:51:16.108480Z","shell.execute_reply.started":"2022-06-06T14:51:15.411112Z","shell.execute_reply":"2022-06-06T14:51:16.107464Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\n\nroc_plot(y_val, m_rf_fit.predict(X_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:16.109865Z","iopub.execute_input":"2022-06-06T14:51:16.110150Z","iopub.status.idle":"2022-06-06T14:51:16.434756Z","shell.execute_reply.started":"2022-06-06T14:51:16.110111Z","shell.execute_reply":"2022-06-06T14:51:16.433849Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n\n# parameters={'n_estimators' : [100,250,500,750],'max_features': [2,5,10,15], 'max_depth':[5,10,15,20,25]}\n# ran_search=RandomizedSearchCV(m_rf , parameters, n_iter=10, scoring=\"f1\", n_jobs = -1).fit(X_train, y_train)\n\n# print(\"The best classifier is: \", ran_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:16.436005Z","iopub.execute_input":"2022-06-06T14:51:16.436264Z","iopub.status.idle":"2022-06-06T14:51:16.440771Z","shell.execute_reply.started":"2022-06-06T14:51:16.436235Z","shell.execute_reply":"2022-06-06T14:51:16.439840Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# print(\"Best cross-validation score: {:.4f}\".format(ran_search.best_score_))\n\n# m_tuned = RandomForestClassifier(criterion=\"gini\", n_estimators = 500, max_features= 5, max_depth = 10).fit(X_train, y_train)\n\n# print(assess_model(y_val,m_tuned.predict(X_val)))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:16.442058Z","iopub.execute_input":"2022-06-06T14:51:16.442312Z","iopub.status.idle":"2022-06-06T14:51:16.454364Z","shell.execute_reply.started":"2022-06-06T14:51:16.442280Z","shell.execute_reply":"2022-06-06T14:51:16.453342Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize = (10,5))\n# roc_plot(y_test, m_tuned.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:16.455887Z","iopub.execute_input":"2022-06-06T14:51:16.456168Z","iopub.status.idle":"2022-06-06T14:51:16.469118Z","shell.execute_reply.started":"2022-06-06T14:51:16.456137Z","shell.execute_reply":"2022-06-06T14:51:16.467844Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"### K-NEAREST NEIGHBOR","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score\n\n# Feature Scaling using StandardScaler\n#\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_val_std = sc.transform(X_val)\n#\n# Fit the model\n#\nknn = KNeighborsClassifier(n_neighbors = 4, p = 2, weights = 'distance', algorithm = 'auto')\nknn.fit(X_train_std, y_train)\n#\n# Evaluate the training and test score\n#\nprint('Training accuracy score: %.3f' % knn.score(X_train_std, y_train))\nprint('Validation accuracy score: %.3f' % knn.score(X_val_std, y_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:16.471144Z","iopub.execute_input":"2022-06-06T14:51:16.472294Z","iopub.status.idle":"2022-06-06T14:51:17.606139Z","shell.execute_reply.started":"2022-06-06T14:51:16.472238Z","shell.execute_reply":"2022-06-06T14:51:17.605133Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ENSEMBLE STACKING","metadata":{}},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom matplotlib import pyplot\n \n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['random_forest'] = RandomForestClassifier(criterion=\"gini\", n_estimators = 500, max_features= 5, max_depth = 10)\n    return models\n \n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)\n    scores = cross_val_score(model, X_train_std, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_val_std, y_val)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:51:17.607614Z","iopub.execute_input":"2022-06-06T14:51:17.607879Z","iopub.status.idle":"2022-06-06T14:52:03.463075Z","shell.execute_reply.started":"2022-06-06T14:51:17.607848Z","shell.execute_reply":"2022-06-06T14:52:03.461987Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# compare ensemble to each baseline classifier\n \n\n# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('svm', SVC()))\n    level0.append(('bayes', RandomForestClassifier(criterion=\"gini\", n_estimators = 500, max_features= 5, max_depth = 10)))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['random_forest'] = RandomForestClassifier(criterion=\"gini\", n_estimators = 500, max_features= 5, max_depth = 10)\n    models['stacking'] = get_stacking()\n    return models\n\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_val_std, y_val)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:52:03.464214Z","iopub.execute_input":"2022-06-06T14:52:03.464462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NEURAL NETWORK MODEL","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import callbacks\n\n# YOUR CODE HERE: define an early stopping callback\nearly_stopping = early_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# define the keras model\nmodel = Sequential()\n\n\nmodel.add(Dense(12, input_dim = 21, activation='relu'))\nmodel.add(Dense(8, activation='sigmoid'))\nmodel.add(Dense(4, activation='sigmoid'))\nmodel.add(Dense(1))\n\n# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\n\nmodel.fit(X_train, y_train, epochs=150, batch_size=100, callbacks=[early_stopping], verbose = 0)\n\n# evaluate the keras model\n_, accuracy = model.evaluate(X_val, y_val)\nprint('Accuracy: %.2f' % (accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('neural_net')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nnew_model = tf.keras.models.load_model('neural_net')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nlil_x = np.asarray(df_test_clean).astype('float32')\n\npredict = new_model.predict([lil_x])\n# print(predict)\n\nprint(np.argmax(predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nmodel1 = keras.Sequential([\n    layers.Dense(12, activation='relu', input_shape = (1,21)),\n    layers.Dense(64, activation='relu'),    \n    layers.Dense(1)\n])\nmodel1.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model1.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=250,\n    epochs=50,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss, val_acc = model.evaluate(X_val, y_val)\nprint(val_loss, val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_loss, val_acc = model.evaluate(X_test, y_test)\n# print(val_loss, val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBOOST","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data\ny_pred = model.predict(X_val)\n\npredictions = [round(value) for value in y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_val, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CATEGORICAL-BOOST","metadata":{}},{"cell_type":"code","source":"X_train_without_pre = df3.drop(columns = [\"PassengerId\", \"Cabin\", \"Name\"])\n\n\nX_train_without_pre['HomePlanet'] = X_train_without_pre['HomePlanet'].replace(['Earth', 'Europa', 'Mars'], [2, 1, 0])\nX_train_without_pre['Destination'] = X_train_without_pre['Destination'].replace(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], [2, 1, 0])\n\nX_train_without_pre\n\nX_train_without_pre['PassengerType'] = X_train_without_pre['PassengerType'].replace(['Single', 'Groups'], [1, 0])\nX_train_without_pre['CabinSide'] = X_train_without_pre['CabinSide'].replace(['Port', 'Starboard'], [1, 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# catboost for classification\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib import pyplot\n\n# evaluate the model\nmodel = CatBoostClassifier(verbose=0, n_estimators=100)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# fit the model on the whole dataset\nmodel = CatBoostClassifier(verbose=0, n_estimators=100)\nmodel.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient boosting","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib import pyplot\n\n# evaluate the model\nmodel = GradientBoostingClassifier()\n\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1234)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n\n# fit the model on the whole dataset\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning hyper param","metadata":{}},{"cell_type":"code","source":"LR = {'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [10,50,100, 150, 200, 250]}\n\ntuning = GridSearchCV(estimator = GradientBoostingClassifier(),\n                     param_grid = LR, scoring = 'r2', cv = None)\n\ntuning.fit(X_train, y_train)\ntuning.best_params_, tuning.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GradientBoostingClassifier()\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state = 1234)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n\n# fit the model on the whole dataset\nmodel = GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 200)\nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TESTING SOFT AND HARD VOTING","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#Common Model Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Defining a list of Machine Learning Algorithms I will be running\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    LGBMClassifier(),\n#     CatBoostClassifier() Running CatBoost causes error\n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Accuracy']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\n# Iterate and store scores in the table\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n    MLA_compare.loc[row_index, 'Accuracy'] = cv_results.mean()\n    \n    row_index+=1\n\n# Present table\nMLA_compare.sort_values(by=['Accuracy'], ascending=False, inplace=True)\nMLA_compare","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier()\n\nboost_grid = {'n_estimators': [100, 500, 1000],\n              'max_depth': [4, 8, 12],\n              'learning_rate': [0.01, 0.05, 0.1, 0.15]}\n\nlgbm_optimal = GridSearchCV(lgbm, boost_grid, scoring = 'accuracy')\nlgbm_optimal.fit(X_train, y_train)\nprint(lgbm_optimal.best_score_)\nprint(lgbm_optimal.best_params_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(eval_metric='logloss')\n\nxgb_optimal = GridSearchCV(xgb, boost_grid, scoring = 'accuracy')\nxgb_optimal.fit(X_train, y_train)\nprint(xgb_optimal.best_score_)\nprint(xgb_optimal.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuned models\nxgb_optimal = XGBClassifier(learning_rate=0.01, max_depth=4, n_estimators=1000, eval_metric='logloss')\nrfm_tuned = RandomForestClassifier(criterion=\"gini\", n_estimators = 500, max_features= 5, max_depth = 10)\nlgbm_optimal = LGBMClassifier(learning_rate=0.05, max_depth=8, n_estimators=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Create Hard Voting Classifier\nEnsemble_HV = VotingClassifier(estimators= [('RFM', rfm_tuned),\n                                           ('XBG', xgb_optimal),\n                                           ('LGBM', lgbm_optimal)],\n                              voting = 'hard')\n\n# Create Soft Voting Classifier\nEnsemble_SV = VotingClassifier(estimators= [('RFM', rfm_tuned),\n                                           ('XBG', xgb_optimal),\n                                           ('LGBM', lgbm_optimal)],\n                              voting = 'soft')\n\n# Return Accuracy Scores\ncv_HV = cross_val_score(Ensemble_HV, X_train, y_train, scoring='accuracy')\ncv_SV = cross_val_score(Ensemble_SV, X_train, y_train, scoring='accuracy')\n\nprint('Hard Voting Classifier:' , cv_HV.mean())\nprint('Soft Voting Classifier:' , cv_SV.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}